**DualMap: Online Open-Vocabulary Semantic Mapping for Natural  Language Navigation in Dynamic Changing Scenes**



# 效果
https://eku127.github.io/DualMap/


![[Pasted image 20251107093107.png]]
给定自然语言查询后（饼干盒），机器人在抽象图的引导下向办公桌移动（1）
发现查询目标缺失（2）
通过导航过程中在线构建的具体图，更新查询目标的状态和新观测结果（如桌子上的饼干盒）
持续更新的地图有助于高效重新选择候选目标，找到移动后的物体（3）
# 一、背景

假设机器人在办公室导航时，比如要解决“**把之前用的苹果笔记本电脑拿过来**“**任务**
需要一个**连接人类语言和动态环境的建图系统**

需具备三项关键能力：
1）**开放词汇性**：在建图过程中必须以**类别无关**的方式**检测目标**，并解读各种自然语言查询（如 “苹果笔记本电脑”），无需依赖固定类别；
2）**高效在线建图**：系统必须**实时增量构建和维护语义图**，以适应环境的持续变化；
3）**动态变化导航**：目标经常改变位置或状态 —— 例如，笔记本电脑可能被移动或打开。系统应采用导航策略，引导智能体找到移动后的目标，并更新地图以支持长期导航。



目前尚无系统能同时满足开放词汇理解、高效在线建图和动态导航三大需求
**DualMap首次满足上述三项要求**



# 二、相关工作

**关注：**
**开放词汇+在线建图+动态处理**
![[Pasted image 20251103195651.png]]


### A.开放词汇在线语义建图

快速封闭集检测器
**YOLO：** 固定类别，无法做到开放词汇（在开放世界的泛化能力）


实现开放词汇感知
两种实现方法

1. 图像标记模型

输入帧生成类别标签+目标检测

ConceptGraphs：成本较高，离线

2. 直接应用类别无关分割+视觉

HOVSG：3D 层面融合，但计算成本高昂
CLIO：地图任务特定，非任务目标需要额外的地图重建


DualMap 
高效、全面的开放词汇映射，在线运行（轻量级目标内检查替代昂贵的 3D 目标间融合）



### B. 动态环境中的建图与导航

这里的动态指的是：场景的变化（而非始终移动的物体）


以往的 3D 语义建图：假设环境是静态的

近期研究探索了语义图适应动态场景变化

OpenIN：
目标移动时更新记忆    离线流程

Khronos：
动态环境实时语义建图
不支持任意语言查询，且缺乏针对移动查询目标的导航策略


DualMap：
双图设计
在线更新记忆，查询目标移动时能高效重新选择导航候选




# PipeLine

![[Pasted image 20251107094710.png]]







# 三、在线具体图

本节详细介绍在线具体图构建（下图）
![[Pasted image 20251103201502.png]]
1. 如何通过快速、全面的开放词汇分割从 RGBD 流中高效生成目标级观测结果
2. 观测结果如何增量更新地图，以确保场景表示的一致性和准确性


##  A. 观测生成

**观测生成流程**
对于每个 RGBD 帧，运行三个并行线程：
a）封闭集检测（如 YOLO）识别列表内目标，但遗漏未见过的目标（如凳子）
b）开放集分割（如 FastSAM）捕获更广泛的目标，但可能存在过分割（如窗户）。
经过优化和融合后，结果被嵌入最终观测集；
c）场景点云
![[Pasted image 20251107103315.png]]

**1.混合开放词汇分割**
采用混合开放词汇分割流程

首先（图 a）
使用 YOLO 快速生成目标级检测结果
> 由于 YOLO 需要预定义类别列表，在运行开始时，根据机器人的工作环境，通过提示大型语言模型生成特定任务的检测列表（例如，“当机器人在办公室运行时，给出常见目标列表”）

此外通过合并具有相似颜色分布的重叠边界框，进一步优化 YOLO 输出

同时（图 b）
运行开放集分割模型 FastSAM,以捕获 YOLO 类别之外的目标

最后
在融合过程中，优先考虑 YOLO 检测结果，保留所有 YOLO 输出，并补充非重叠的 FastSAM 片段


**这种混合策略确保了下游建图的全面（所有潜在目标）、高效目标分割**

**2.语义特征嵌入**
对图像中每个检测到的目标，得到一个**语义特征向量**，这个特征同时包含：
- 视觉信息
- 文本信息

图像特征提取：`f_image`
对于每个目标
- 先用检测框（bounding box）裁剪目标所在区域；
- 然后用 **CLIP** 模型的图像编码器把裁剪后的区域编码成一个向量：`f_image`

文本特征提取：`f_text`
如果检测器能提供类别标签（例如“chair”），把这个类别词输入到 CLIP 的文本编码器中，得到：`f_text`

最后，把视觉和文本特征线性加权求和，得到最终的语义特征：

$$ f = w_{\text{image}} f_{\text{image}} + w_{\text{text}} f_{\text{text}} $$

其中视觉特征占 70%，文本特征占 30%：
$$
w_{\text{image}} = 0.7；w_{\text{text}} = 0.3
$$

**3.观测结构**

定义每一帧的观测集合为：
$$
Z = \{z_1, z_2, \ldots, z_N\}
$$
其中 N 是当前帧中检测到的物体（或分割区域）的数量。

每一个观测 z 由四个部分组成：
$$
z = (P_z, f_z, y_z, t_z)
$$

| 符号    | 含义              | 来源                                                |
| ----- | --------------- | ------------------------------------------------- |
| $P_z$ | 当前物体的**3D点云**   | 从深度图中把对应的目标区域投影到三维世界坐标系得到（使用相机内参和当前位姿）            |
| $f_z$ | 当前物体的**语义特征向量** | CLIP 融合图像特征和文本特征                                  |
| $y_z$ | 当前物体的**类别标签**   | 来自 YOLO 检测器的分类结果；若 FastSAM 分割出物体但未识别类别，则设为 “null” |
| $t_z$ | 当前物体的**时间戳**    | 表示该观测被创建的时刻，用于时间管理和地图更新                           |

**流程**
1. 输入当前帧图像 *I*
2. **FastSAM** → 分割出多个区域（segments）。
3. 对每个区域：
- **从深度图**投影到世界坐标 → 得到该区域的点云$P_z$
- **YOLO分类器**判断类别 → 得到标签$f_z$
- **CLIP编码器**提取视觉+文本语义特征 → 得到 $y_z$
- 记录时间戳$t_z$
4. 最终得到一个结构化的观测集合 $Z_{i}$，里面包含当前帧中所有目标的三维语义信息。


 **4.场景点云生成**（图c)

为了**支持下游的导航任务**,需要能反映**整个环境**几何结构的“场景点云”（scene point cloud）$P_s$
> 区别上面单个物体的点云

**做法：**

1. 选择位姿变化显著的 RGBD 帧（平移 > 1.0 米或旋转 > 20°）
> 避免重复采样相似视角的数据，提高效率

2. 投影融合：把该帧的深度图（Depth Map）投影到世界坐标系中，得到当前帧的3D点云，然后将这些点云**合并进全局点云**。
3. 下采样：合并过程中，做点云下采样，减少点数量、压缩数据量
4. 低频并行线程：后台并行执行，**低频率地更新场景点云**,不会占用太多实时计算资源

## B. 目标关联与更新

**1.地图初始化**

具体图$M_c$表示**目标集合**,定义为$M_c = \{o_1, o_2, \ldots\}$,每个目标 o 的结构为$(P_o, y_o, f_o, L_o)$
> 以**目标为单位**存储的

其中$L_o$记录关联的观测列表,即哪些观测贡献了该对象

当接收到第一个观测集时，初始化具体图。对于每个观测在地图中创建新目标 o

**2.观测匹配**
新一帧的观测$Z = \{z_1, z_2, \ldots, z_N\}$如何与已有地图中的对象$M_c = \{o_1, o_2, \ldots, o_M\}$进行对应或更新。
要做的事是：
> 对每个新的观测 $z_i$，找到它在地图中最匹配的对象 $o_j$，并决定是更新已有对象还是新建一个对象。
> 这里观测和对象 都是指的单个物体


匹配依赖两个核心特征：几何特征$P$和语义特征$f$
这两个信息结合可以防止以下情况：
- 仅靠几何匹配 → 可能混淆形状相似但语义不同的物体（例如“桌子”和“地面”）；
- 仅靠语义匹配 → 可能误将不同位置的“椅子”当作同一个。


系统构建一个相似度矩阵$S(z_i, o_j)$,表示第 $i$个观测和第 $j$ 个对象的匹配得分。
计算公式：
$$S(z_i, o_j) = \cos(f_{z_i}, f_{o_j}) + \text{Overlap}(P_{z_i}, P_{o_j})$$

| 项                                 | 含义                            |
| --------------------------------- | ----------------------------- |
| $\cos(f_{z_i}, f_{o_j})$          | 语义特征的余弦相似度。越接近1表示语义越相似。       |
| $\text{Overlap}(P_{z_i}, P_{o_j}$ | 点云的重叠率（几何匹配度），表示两者在3D空间中重合程度。 |

对于每个新观测 $z_i$，找到地图中得分最高的对象：
$$o^* = \arg\max_{o_j} S(z_i, o_j)$$
即寻找“当前帧中这个观测最可能属于地图中哪个对象”

$$
S(z_i, o^*) \geq \tau
$$


- 若超过阈值 (匹配成功)：
  - 认为 $z_i$  与  $o^*$ 是同一个物体；
  - 对象 $o^*$  被更新：
    - **语义特征更新**：
$$
    f_{o^*} \leftarrow \text{weighted average of } (f_{o^*}, f_{z_i})
    $$
  权重由已有观测数量  $|L_{o^*}|$ 决定；
	 - **几何更新**：
  
$$
    P_{o^*} \leftarrow P_{o^*} \cup P_{z_i}
    $$
  把新点云并入旧的
	 - **观测记录更新**：
    $$
    L_{o^*} \leftarrow L_{o^*} \cup \{z_i\}
    $$

- 若低于阈值 (匹配失败)：
  - 认为这是一个新物体；
  - 新建一个对象，加入地图中：
    $$
    O_{new} = (P_{z_i}, y_{z_i}, f_{z_i}, \{z_i\})
    $$
  



# 四、抽象图与导航

本节详细介绍如何从具体图$M_c$构建抽象图$M_a$(图b),以及这种双图设计如何支持动态目标变化下的语言引导导航(图c)
![[Pasted image 20251107125019.png]]

![[Pasted image 20251107125036.png]]


##  A. 地图抽象

前面的具体图$M_c$
- 地图中包含许多冗余的、小的、易变的物体（例如杯子、书、笔等）；
- 这些物体会频繁变化（被移动），不适合作为稳定的空间参考。

抽象图$M_a$目的
把地图简化成一种**语义层次的场景结构表示**，保留对导航最有用的核心元素。


1.识别 Anchor Object（锚定物体）  Volatile Object（易变物体）
- **Anchor objects（锚定物体）**：通常不会移动的、稳定的大件（如桌子、床、墙等）。
- **Volatile objects（易变物体）**：经常被移动的、小件（如杯子、书、笔等）。

首先使用两个代表性锚点和易变类别列表对目标进行分类，对于每个目标o，利用 CLIP 特征计算其与两个列表的相似度。如果与一个列表的最大相似度比另一个高出0.05，则将该目标分配到对应的类别

2.建立 Volatile 与 Anchor 的空间关系
对于易变物体（Volatile），舍弃其几何与观测记录（因为它会动），只保留与锚定物体相关的语义关系。
论文中专注于 **“on” 关系**，也就是判断一个物体是否放在另一个上面。

3.建立关系存储
建立空间关系后，将 volatile 的语义特征 $f_v$​ 存入对应 anchor 的“附属语义列表”
$$ L_a = \{f_{v1}, f_{v2}, \ldots, f_{vK}\}$$

最终每个锚定物体 a 的结构为：
$$a = (P_a, f_a, y_a, L_a)$$

而整个抽象地图由所有 anchor 对象组成：
$$M_a = \{a_1, a_2, \ldots\}$$


4.场景点云提取
为支持导航任务，我们简化锚点目标的几何结构，仅保留其 2D 投影点云。
1️⃣ **从场景点云 $P_s$**
2️⃣ **投影到鸟瞰平面（Bird’s Eye View）**  
3️⃣ **离散化成栅格（spatial bins）**  
4️⃣ **统计每个格点的点云密度直方图**，并取第90百分位作为阈值 → 构建二值占据图（binary occupancy map）
 5️⃣ **该布局仅在发出新的导航请求时更新**。

## B.候选检索

在拥有**抽象语义地图**后，如何根据**自然语言查询（自然语言指令 Q）**   **来找到最可能对应的导航目标位置**。（从语义地图中找到你说的那个地方/物体在哪里）

1.查询编码
使用 CLIP 把语言输入 $Q$ 编码成一个语义嵌入向量
$$f_q = \text{CLIP_TextEncoder}(Q)$$
这个 $f_q$表示自然语言在**语义空间**中的特征

2.计算 Anchor 匹配得分
对于每个锚定物体 a
系统计算该 anchor 与查询语义的匹配度，考虑两种可能：
1. 查询可能直接指向 anchor 本身（例如 “go to the bed”）；
2. 查询可能指向 anchor 上的易变物体（例如 “go to the table with the cup”）。

因此，对每个锚点 a，它的得分定义为：
$$s(a) = \max \left( \cos(f_q, f_a), \max_i \cos(f_q, f_{v_i}) \right)$$

计算所有 anchor 的分数后，选择得分最高的那个
并将该对象位置作为**导航目标（navigation target）**。  
后续模块会基于该 anchor 的三维坐标或场景点云来执行路径规划。


## C. 导航策略
确定目标语义对象 $a^*$ 后，**如何实际规划路径并动态更新导航目标** 的策略。

首先
在抽象地图上规划通往目标的全局路径

沿该路径移动时
增量构建局部具体图并并评估局部具体图中的目标是否可能匹配；在视为可靠匹配。然后通过规划局部路径到达目标。

如果在目标$a^*$附近未找到可靠匹配，表明查询目标可能已改变位置，系统将在更新后的抽象图上重新执行候选检索，选择新的锚点。

## D. 地图更新机制

从局部地图提取新的锚点

将每个新锚点中的所有现有锚点 a 进行比较，计算几何重叠得分

$$S_{\text{overlap}} = \text{Overlap}(P_{a_{\text{new}}}, P_a)''$$

1)重叠度较高
认为是同一个物体，但只是新的观测
于是执行**融合更新：**
![[Pasted image 20251107134513.png]]

2）重叠度低
直接把它作为新的 anchor 插入地图